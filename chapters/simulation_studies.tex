\graphicspath{{fig/sim_studies/}}

\chapter{Simulation studies}
\label{cha:sim_studies}

The feasibility of performing parameter inference on real data can be assessed by
simulation study. In simulation studies, candidate models are forward simulated for known
parameter values. This simulated data is then used to verify that the known parameter
values can be recovered by inference.

If parameter inference is \emph{not} possible on simulated data, we know that the same
inference will not be possible on real data either. Simulation studies also provide a good
opportunity to assess (and frankly, debug) our implementation of MCMC algorithms
constructed to target the posterior distribution.

\section{Global models}

Global models refer to models in which all agents take the \emph{same} parameter values.
These are in contrast to hierarchical models, in which all agents take \emph{different}
parameter values.  We shall first seek to perform simulation studies on global
models, as they represent a simpler inference problem.  

Recall from Bayes' Theorem (\cref{eq:bayes_theorem}) that to realise the posterior
distribution we need to combine the likelihood of the data with our prior beliefs. Let us
begin by considering the likelihood of observing a single agent updating its direction, in
the presence of neighbours, from $\theta_{i, t}$ to $\theta_{i, t+1}$.  From
\cref{eq:students_update} we have that agent $i$ updates its direction as a draw from the
generalised Students $t$-distribution with location $\angmean{\theta}_{i,t}$, scale
$\sigma_Y$ and degrees of freedom $\nu$. As such, the likelihood of observing this
directional update can be quantified as:
\begin{equation*}
    L(\sigma_Y, \nu, \angmean{\theta}_{i,t} \given \theta_{i,t+1}) = 
    \frac{\Gamma(\frac{\nu +1 }{2})}{\Gamma(\frac{\nu}{2})\sqrt{\pi\nu}\sigma_Y}
        \Bigg(1 + \frac{1}{\nu}
                  \bigg(\frac{\theta_{i,t+1}-\angmean{\theta}_{i,t}}{\sigma_Y}\bigg)^2
        \Bigg)^{-\frac{\nu+1}{2}},
\end{equation*}
where $\Gamma$ is the gamma function. Building on this, let us consider the likelihood of
observing agent $i$'s directional updates from time $t$ to time $t+2$. As the noise
experienced by an agent is independent of the noise it experienced at previous time
steps, we may express this likelihood as a product
\begin{align*}
    L(\sigma_Y,\nu,\angmean{\theta}_{i,t},\angmean{\theta}_{i,t+1}\given\theta_{i,t+1},
    \theta_{i,t+2})
    = L(\sigma_Y,\nu,\angmean{\theta}_{i,t+1}\given\theta_{i,t+2})\times
    L(\sigma_Y,\nu,\angmean{\theta}_{i,t}\given\theta_{i,t+1}).
\end{align*}

In general, we wish to express the likelihood of observing agent $i$'s directional updates
over any number of times steps. Suppose then that we observe the directions of agent $i$
from time $t=1$ to time $t=T$. Again, as consecutive realisations from the noise
distribution are independent, we can compute the likelihood as a product
\begin{align*}
    L(\sigma_Y, \nu, \angmean{\theta}_{i,1:T-1} \given \theta_{i,2:T})
    =& \prod_{t=1}^{T-1} L(\sigma_Y,\nu,\angmean{\theta}_{i,t} \given \theta_{i,t+1}) \\
    =& \prod_{t=1}^{T-1} 
        \frac{\Gamma(\frac{\nu +1 }{2})}{\Gamma(\frac{\nu}{2})\sqrt{\pi\nu}\sigma_Y}
        \Bigg(1 + \frac{1}{\nu}
                  \bigg(\frac{\theta_{i,t+1}-\angmean{\theta}_{i,t}}{\sigma_Y}\bigg)^2
        \Bigg)^{-\frac{\nu+1}{2}},
\end{align*}
where $\angmean{\theta}_{i,1:T-1}$ is shorthand for
$\angmean{\theta}_{i,1},\angmean{\theta}_{i,2},\ldots,\angmean{\theta}_{i,T-1}$, and
similarly for $\theta_{i,2:T}$. Finally, although we have expressed the likelihood of
observing a single agent's directional changes over $T$ observations, we really wish to
express the likelihood of observing \emph{an entire flock's} directional changes over $T$
observations. Consider observing a flock of $N$ individuals over $T$ time steps. The
likelihood of observing their directional updates can be quantified as:
\begin{align}
    \label{eq:likelihood}
    \begin{split}
    L(\sigma_Y, \nu, \angmean{\theta}_{1:N,1:T-1} \given\theta_{1:N,2:T}) 
    &= \prod_{i=1}^N \prod_{t=1}^{T-1} L(\sigma_Y,\nu,\angmean{\theta}_{i,t}\given\theta_{i,t+1})  \\
    &= \prod_{i=1}^N \prod_{t=1}^{T-1} 
        \frac{\Gamma(\frac{\nu +1 }{2})}{\Gamma(\frac{\nu}{2})\sqrt{\pi\nu}\sigma_Y}
        \Bigg(1 + \frac{1}{\nu}
                  \bigg(\frac{\theta_{i,t+1}-\angmean{\theta}_{i,t}}{\sigma_Y}\bigg)^2
        \Bigg)^{-\frac{\nu+1}{2}}.
    \end{split}
\end{align}

The likelihood function for all our models takes the same form. What differs between
models is the specification of the weighting function $\omega_{ij,t}$ and the
corresponding computation of $\angmean{\theta}_{i,t}$.

\subsection{Vicsek model}

We simulate the Vicsek model to realise a flock of $N=45$ agents moving for $T=200$ time
steps. We desire to work with simulated data which is similar to real data. As such, the
initial conditions for this simulation are taken from an observation of a real flocking
event. Parameters $r=50$, $\sigma_Y=0.03$ and $\nu=7$ are set for this simulation.
\cref{fig:vicsek_sim_study} shows the data generated by this simulation. We seek to assess
whether we can recover the true parameter values from the simulated data.

\begin{figure}[tb]
    \includegraphics{vicsek_sim.pdf}
    \caption{The trajectories of $N=45$ agents simulated over $T=200$ time steps according to
    the Vicsek model. The initial conditions of the simulation reflect that of a real
flocking event. The parameters of the model are given by $r=50$, $\sigma_Y=0.03$ and
$\nu=7$.}
    \label{fig:vicsek_sim_study}
\end{figure}

The likelihood of observing the data can be quantified by \cref{eq:likelihood}. However,
to target the posterior we also need to specify our prior beliefs about the model
parameters. To allow the data to drive the inference, we shall specify weakly informative
priors. Once realisations from the posterior have been made, we overlay our priors to
assess how our beliefs have updated in light of the data. 

Our prior beliefs about likely values for the degrees of freedom parameter $\nu$ shall be
reflected by a $\Ga(2, 0.1)$ distribution, as popularised by \textcite{juarez10}.  As $r$
and $\sigma_Y$ both represent strictly positive measures, we shall use a gamma
distribution to quantify our beliefs about them. These prior beliefs, as detailed in
\cref{eq:vicsek_priors} and visualised in \cref{fig:vicsek_priors}, represent vague
beliefs, allowing a large range of possible parameter values.
\begin{align}
    \label{eq:vicsek_priors}
    \begin{split}
        r           & \sim \Ga(50, 1) \\
        \sigma_Y    & \sim \Ga(2, 100)\\
        \nu         & \sim \Ga(2, 0.1)
    \end{split}
\end{align}
\begin{figure}[tb]
    \includegraphics{vicsek_priors.pdf}
    \caption{Probability density functions representing our prior beliefs about plausible
    parameter values of the Vicsek model. These distributions represent
    weakly-informative priors, allowing a large range of possible parameter values.}
    \label{fig:vicsek_priors}
\end{figure}

Having specified the likelihood function, and quantified our prior beliefs about the model
parameters, we now have all the ingredients necessary to target the posterior.
Unfortunately, the discontinuity in the weighting function of the Vicsek model results in
a discontinuous posterior. As Stan requires a continuously differentiable posterior to
implement its NUTS algorithm, we instead target the posterior distribution with a random
walk Metropolis--Hastings sampler.

We implement the random walk sampler for $1,100,000$ iterations. The initial $100,000$
iterations of the scheme are discarded to allow a burn-in period. The chains were
initialised as draws from our prior beliefs. The output of this scheme is summarised in
\cref{tab:vicsek_sim_study_summary}.

\cref{fig:vicsek_sim_study_chains} shows the chains generated by our inference scheme,
having discarded the initial burn-in period. From these trajectories we see evidence that
our sampler has converged: the chains oscillate around fixed values with common
variance.  The samples generated by these chains are visualised in
\cref{fig:vicsek_sim_study_hists}.  The true parameter values used to generate the data
are shown by green vertical lines. See that in each posterior the true value is
successfully captured by our posterior beliefs, and lies close to the posterior mode.
Moreover, our prior beliefs are overlain in red. Our prior beliefs appear flat in
comparison to our posteriors, reflecting that our beliefs have updated considerably in
light of the data.
\begin{table}[tb]
\begin{tabular}{@{}lrrrrr@{}}
\toprule
Parameter &   mean &    sd &    5\% &   95\% & $N_{\textrm{eff}}$ \\
\midrule
$r$          & 50.019 & 0.041 & 49.992 & 50.112 &            12\,290 \\
$\sigma_{Y}$ &  0.030 & 0.000 &  0.029 &  0.030 &            44\,560 \\
$\nu$        &  7.098 & 0.511 &  6.310 &  7.985 &            47\,420 \\
\bottomrule
\end{tabular}
\caption{Summarising the posterior realisations for the parameters inferred in fitting the
    Vicsek model to simulated data. Realisations were made by implementing a random walk
    Metropolis--Hastings sampler. Columns report the posterior mean and standard deviation
    of each parameter. In addition to this, the fifth and ninety-fifth percentiles of our
    posteriors are quantified. Finally, the effective sample size $N_{\textrm{eff}}$ of
    each posterior is computed.}
    \label{tab:vicsek_sim_study_summary}
\end{table}
\begin{figure}[tb]
    \includegraphics{mh_r_trace.pdf}
    \caption{Trajectories of the chains over each model parameter, generated by our
    implementation of a random walk Metropolis--Hastings sampler. The green lines
    represent the true parameter values used to generate the simulated data. The chains
    look well-behaved, showing no irregularities, and suggest that our sampler has
    converged.}
    \label{fig:vicsek_sim_study_chains}
\end{figure}
\begin{figure}[tb]
    \includegraphics{mh_r_hist.pdf}
    \caption{Distributions of the samples realised from the posterior distribution. The
    green lines represent the known parameter values we seek to recover. Prior beliefs are
    overlain in red. Our posterior beliefs are seen to accurately capture the true
    parameter values. The data can be seen to have been very informative, as our posterior
    beliefs have updated considerably from our prior beliefs.}
    \label{fig:vicsek_sim_study_hists}
\end{figure}

Considering the output of this simulation study, we conclude that we can accurately
capture the true parameter values of data simulated from the Vicsek model. This gives us
confidence in moving forward to fit this model to real data.

\subsection{Continuous models}

We have demonstrated that we can fit the Vicsek model to simulated data.  We now seek to
fit the power-law weighted (\cref{eq:power_law_interaction}) and Gaussian weighted models
(\cref{eq:gaussian_interaction}) to simulated data.

As these models implement continuous interaction rules, we may attempt parameter inference
using the Stan programming language. \cref{eq:likelihood} can be used to quantify the likelihood of
observing simulated data. To target the posterior we combine this likelihood with our prior
beliefs.

The power-law and Gaussian weighted models represent similar interaction rules. Both
interactions are parameterised by strictly positive measures. As with the parameters
$\nu$, $\sigma_Y$ and $r$, we shall use gamma distributions to quantify our prior beliefs
about $\alpha$ and $\sigma_X$.  To ensure that our prior beliefs between these models are
consistent, we shall determine our priors by considering plausible values of the weighting
$\omega_{ij,t}$. Using information about $d_{ij,t}$, along with
\cref{eq:power_law_interaction,eq:gaussian_interaction}, we can use our beliefs about
$\omega_{ij,t}$ to derive prior beliefs about $\alpha$ and $\sigma_X$.

To realise our prior beliefs we shall make two probabilistic statements about
$\omega_{ij,t}$. These can be used to determine prior distributions which best reflect
these statements. First, we consider the weighting which agent $i$ gives to its closest
neighbour at time $t$. We believe that the probability that this weighting is less than or
equal to $0.25$ is $0.025$. Similarly, we believe with probability $0.975$ that the
weighting which agent $i$ gives to its fifth closest neighbour will be less than or equal
to $0.9$.  More concisely, we can express these statements as:
\begin{align*}
    P(\omega_{ij, t}({\min_j(d_{ij,t}, 1)}) \leq 0.25) &= 0.025, \\
    P(\omega_{ij, t}({\min_j(d_{ij,t}, 5)}) \leq 0.90) &= 0.975,
\end{align*}
where $\omega_{ij, t}({\min_j(d_{ij,t}, k)})$ is the influence of agent $i$'s $k$-th
closest neighbour at time $t$. A simple numerical optimisation scheme can be used to
determine shape and scale parameters of our gamma priors, by minimising the function:
\begin{equation*}
    [P(\omega_{ij, t}({\min_j(d_{ij,t}, 1)}) \leq 0.25) - 0.025]^2
    + [P(\omega_{ij, t}({\min_j(d_{ij,t}, 5)}) \leq 0.90) - 0.975]^2 = 0.
\end{equation*}
With this we realise prior beliefs:
\begin{align}
    \label{eq:gauss_power_priors}
    \begin{split}
        \alpha   & \sim \Ga(2, 2) \\
        \sigma_X & \sim \Ga(5, 5).
    \end{split}
\end{align}

\begin{figure}[tb]
    \includegraphics{gauss_power_priors.pdf}
    \caption{Probability density functions representing our prior beliefs about $\alpha$
    and $\sigma_X$, the interaction parameters of the power-law and Gaussian weighted
    models respectively. These prior beliefs are also given in
    \cref{eq:gauss_power_priors}.}
\end{figure}

\subsubsection{Power-law weighted}

The initial conditions for our simulated data are taken from an observation of a real
flocking event. A realistic-looking flock is generated with parameters $\alpha=1.5$,
$\sigma_Y=0.03$ and $\nu=7$.  \cref{fig:power_sim_study} shows the trajectories realised
by this simulation. We then seek to recover the known parameter values by inference.

\begin{figure}[tb]
    \includegraphics{power_sim.pdf}
    \caption{Simulated data generated by a run of the power-law weighted model with
    parameter values $\alpha=1.5$, $\sigma_Y=0.03$ and $\nu=7$. The data represents the
    trajectories of motion of $N=45$ agents moving for $200$ frames. Initial conditions
    for this simulation were realised from an observation of a real flocking event.}
    \label{fig:power_sim_study}
\end{figure}

Stan is used to perform parameter inference on this model. Four independent chains are
initialised at realisations from our prior beliefs. Each chain is the simulated for
$10,000$ iterations, with the first $5000$ iterations discarded as a warm-up period.

\begin{table}[tb]
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
Parameter &  mean &    sd & 5\% & 95\% & $N_{\textrm{eff}}$ & $\hat{R}$ \\
\midrule
$\alpha$ & 1.507 & 0.018 &   1.476 &    1.536 &            13\,130 &       1.0 \\
$\sigma_{Y}$ & 0.03 & 0.0 &   0.029 &    0.031 &            11\,395 &       1.0 \\
$\nu$ & 7.09 & 0.504 &   6.278 &    7.913 &            11\,575 &       1.0 \\
\bottomrule
\end{tabular}
\caption{Summarising the posterior realisations made by Stan. Each row represents a
parameter of the power-law weighted model, and our posterior beliefs about it.}
\label{tab:power_sim_study_summary}
\end{table}

\begin{figure}[tb]
    \includegraphics{stan_power_trace.pdf}
    \caption{}
    \label{fig:power_sim_study_chains}
\end{figure}

\begin{figure}[tb]
    \includegraphics{stan_power_hist.pdf}
    \caption{}
    \label{fig:power_sim_study_hists}
\end{figure}

\subsubsection{Gaussian weighted}

\begin{figure}[tb]
    \includegraphics{gauss_sim.pdf}
    \caption{}
\end{figure}

\begin{table}[tb]
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
Parameter &  mean &   sd & 5\% HPD & 95\% HPD & $N_{\textrm{eff}}$ & $\hat{R}$ \\
\midrule
$\sigma_{X}$ & 19.938 & 0.203 &  19.601 &   20.265 &            13\,310 &       1.0 \\
$\sigma_{Y}$ & 0.025 & 0.0 &   0.024 &    0.025 &            11\,700 &       1.0 \\
$\nu$ & 7.245 & 0.53 &   6.406 &    8.118 &            11\,360 &       1.0 \\
\bottomrule
\end{tabular}
\caption{}
\end{table}

\begin{figure}
    \includegraphics{stan_gauss_trace.pdf}
    \caption{}
\end{figure}

\begin{figure}
    \includegraphics{stan_gauss_hist.pdf}
    \caption{}
\end{figure}

\subsection{Topological model}

\section{Hierarchical models}
