\graphicspath{{fig/bayes_intro/}}

\chapter{Bayesian statistics}
\label{cha:bayes_intro}
In this thesis we utilise techniques from Bayesian inference to fit theoretical models of collective 
behaviour to real data. Bayesian inference allows the practitioner to capture uncertainty about 
fitted model parameters. In addition to this the Bayesian framework permits flexible model structures 
and potential inclusion of expert information via the prior distribution. With this we seek to fit 
empirical data to a generalisation of a popular model from the literature.

In this chapter we shall introduce and give overviews of some important concepts of Bayesian 
inference, outline schemes which can be used to infer model parameters, and discuss some of the 
problems which may arise and how we might address them.

\section{Bayesian inference}
\label{sec:bayesian_inference}
Using Bayesian inference we wish to quantify beliefs and uncertainties about parameters $\bm{\theta} 
= (\theta_1, \theta_2,\dots,\theta_p)^T$, using data $\bm{x}$ which we observe. Given this observed 
data, the likelihood function for the parameters is defined
\[
    L(\bm{\theta}\given\bm{x}) = f(\bm{x} \given \bm{\theta}).
\]
The likelihood quantifies the probability distribution of the data in terms of the parameters. We may 
then specify our prior knowledge about the parameters $\bm{\theta}$ through the prior distribution 
$\pi(\bm{\theta})$. Bayes Theorem can then be used to incorporate both the likelihood function and 
our prior beliefs, to form the posterior distribution
\begin{equation}
\label{eq:bayes_theorem}
    \pi(\bm{\theta}\given\bm{x}) = 
\frac{\pi({\bm{\theta}})L(\bm{\theta}\given\bm{x})}{\int_{\bm{\theta}} 
\pi(\bm{\theta})L(\bm{\theta}\given\bm{x}) \, \textup{d}\bm{\theta}}.
\end{equation}
Because the integral in the denominator is not a function of $\bm{\theta}$, we may consider it a 
constant of proportionality and express our posterior beliefs as proportional to the product of the 
likelihood and prior, that is
\begin{align*}
    \pi(\bm{\theta}\given\bm{x}) &\propto \pi(\bm{\theta}) \times L(\bm{\theta}\given\bm{x})\\
    \text{posterior} &\propto \text{prior} \times \text{likelihood}
\end{align*}

\section{Markov chain Monte Carlo (MCMC)}
\label{sec:mcmc}
For the most part, the normalising constant (given in the denominator of \cref{eq:bayes_theorem}) 
will have multiple dimensions, not produce a density function of standard form, and be difficult to 
evaluate in all but the most trivial cases. Markov chain Monte Carlo algorithms provide methods to 
sample from the targeted density $\pi(\bm{\theta} \given \bm{x})$, whilst avoiding evaluating the 
troublesome normalising constant.

\subsection{Gibbs sampling}
\label{ssec:gibbs_sampling}
One may use the full conditional distributions of parameters to sample from a multivariate density. 
Doing so is to implement the Gibbs algorithm. So, instead of sampling from the full posterior, we 
sample from the conditional posteriors of the parameters one at a time. The Gibbs algorithm is useful 
when the conditional densities can be expressed in standard form and are easy to sample form.

Say we wish to target the density $\pi(\bm{\theta})$ where $\bm{\theta} = (\theta_1, \theta_2, \dots, 
\theta_p)^T$, where the full conditional densities are $\pi(\theta_i \given \theta_1, \theta_2, 
\dots, \theta_{i-1}, \theta_{i+1}, \dots, \theta_p)$, for $i=1,\dots,p$, then we may use the Gibbs 
sampler, as described in \cref{alg:gibbs}.

\begin{algorithm}[htbp]
	\begin{enumerate}
	    \setcounter{enumi}{-1}
	    \item Initialise chain with $\bm{\theta}^{0}$. Set $j=1$.
	    \item Generate $\bm{\theta}^{j}$ from $\bm{\theta}^{j-1}$ by simulating from:
	    \begin{align*}
	            {\theta_1}^{j} &\sim \pi({\theta_1}^{j} \given 
{\theta_2}^{j-1},\dots,{\theta_p}^{j-1},\bm{x})\\
	            {\theta_2}^{j} &\sim \pi({\theta_2}^{j} \given {\theta_1}^{j}, {\theta_3}^{j-1}, \dots, 
{\theta_p}^{j-1},\bm{x})\\
	            &\hspace{0.25cm}\vdots \\
	            {\theta_p}^{j} &\sim \pi({\theta_n}^{j} \given {\theta_1}^{j}, \dots, 
{\theta_{p-1}}^{j-1},\bm{x})
	    \end{align*}
	    \item Increment $j$ to $j+1$. Repeat from step $1$.
	\end{enumerate}
	\caption{Gibbs Algorithm targeting the density $\pi(\bm{\theta} \given \bm{x})$.}
	\label{alg:gibbs}
\end{algorithm}

\subsection{Metropolis-Hastings}
\label{ssec:metropolis_hastings}
The Metropolis-Hastings algorithm is another MCMC scheme. The algorithm was introduced by 
\cite{metropolis53}, and this work was later generalised by \cite{hastings70}. The algorithm works by 
constructing a Markov chain which has stationary distribution equivalent to the distribution of 
interest.

% Add thin space after \min ???
\begin{algorithm}[htbp]
	\begin{enumerate}
	    \setcounter{enumi}{-1}
	    \item Initialise chain with $\bm{\theta}^{0}$. Set $j=1$.
	    \item Propose ${\bm{\theta}}^\star$ by sampling from $q(\cdot \given {\bm{\theta}}^{j-1})$, 
where $q$ is some proposal distribution
	    \item Construct the acceptance probability $\alpha({\bm{\theta}}^\star \given 
{\bm{\theta}^{j-1}})$ as
	    \begin{equation*}
			\alpha({\bm{\theta}}^\star \given \bm{\theta}) = \min\bigg\{ 1, 
\frac{\pi({\bm{\theta}}^\star)}{\pi({\bm{\theta}}^{j-1})} \frac{L(\bm{\theta}^\star \given 
\bm{x})}{L({\bm{\theta}}^{j-1} \given \bm{x})} \frac{q({\bm{\theta}}^{j-1} \given 
{\bm{\theta}}^\star)}{q({\bm{\theta}}^\star \given {\bm{\theta}}^{j-1})} \bigg\}.
	    \end{equation*}
	    \item With probability $\alpha({\bm{\theta}}^\star \given {\bm{\theta}^{j-1}})$ set 
${\bm{\theta}}^j = {\bm{\theta}}^\star$, otherwise set ${\bm{\theta}}^j = {\bm{\theta}}^{j-1}$.
	    \item Increment $j$ to $j+1$. Repeat from step $1$.
	\end{enumerate}
	\caption{The Metropolis-Hastings used to target the posterior distribution $\pi(\bm{\theta} \given 
\bm{x})$.}
	\label{alg:metropolis_hastings}
\end{algorithm}

The algorithm begins by initialising the chain with parameters $\bm{\theta}^{0}$. Next, the algorithm 
proposes new values ${\bm{\theta}}^\star$ from a proposal distribution, $q(\bm{\theta}^\star \given 
{\bm{\theta}}^{j-1})$, which is chosen to have the same support as the target distribution. After 
this, the proposed values ${\bm{\theta}}^\star$ are either accepted or rejected, depending on the 
evaluation of the acceptance probability $\alpha({\bm{\theta}}^\star \given {\bm{\theta}^{j-1}})$. 
Because the acceptance probability depends on a ratio of $\pi(\cdot \given \bm{x})$, the normalising 
constants cancel and therefore the target distribution only has to be known to a constant of 
proportionality. Metropolis-Hastings is described more formally in \cref{alg:metropolis_hastings}.

\subsubsection{Choosing a Proposal Distribution}
\label{ssec:proposal_distribution}
The practitioner must choose a suitable proposal distribution $q(\bm{\theta}^\star \given 
\bm{\theta})$. Ideally the choice of proposal distribution will give rapid convergence to 
$\pi(\bm{\theta} \given \bm{x})$ and efficiently explore the support of $\pi(\bm{\theta} \given 
\bm{x})$.

A special case of Metropolis-Hastings arises when the proposal distribution is symmetric, that is
\begin{equation*}
	q(\bm{\theta}^\star \given \bm{\theta}) = q(\bm{\theta} \given \bm{\theta}^\star).
\end{equation*}
In this case we observe cancellation in the acceptance ratio, as it simplifies to become 
\begin{equation*}
	\alpha({\bm{\theta}}^\star \given \bm{\theta}) = \text{min}\bigg\{ 1, 
\frac{\pi({\bm{\theta}}^\star)}{\pi({\bm{\theta}}^{j-1})} \frac{L(\bm{\theta}^\star \given 
\bm{x})}{L({\bm{\theta}}^{j-1} \given \bm{x})} \bigg\}.
\end{equation*}

Another special case of Metropolis-Hastings is the random walk sampler. In this case proposals are 
realised as
\begin{equation*}
	\bm{\theta}^\star = \bm{\theta}^{j-1} + \bm{\omega}^{j-1},
\end{equation*}
where the $\bm{\omega}$ are drawn from
\begin{equation*}
	\bm{\omega}^{j-1} \sim \mathcal{N}_p(\bm{0}, \Sigma),
\end{equation*}
and $\mathcal{N}_p$ denotes a $p$--dimensional multivariate normal distribution. The parameter 
$\Sigma$ is called the tuning parameter and controls how the chain moves around the parameter space. 
Mixing describes how efficiently the chain moves around the sample space and how long it takes for 
the chain to converge to the target distribution.

Crucially then, the parameter $\Sigma$ controls the mixing of the chain. So, naturally, we wish to 
select some optimum $\Sigma$ to try and improve mixing. If the target distribution is Gaussian, it 
has been shown that 0.234 is an optimum acceptance probability \parencite{roberts01}. In an attempt 
to tune $\Sigma$ to obtain the optimum acceptance probability, a common technique is to use 
\begin{equation*}
	\Sigma = \frac{2.38^2}{p} \widehat{\text{Var}}(\bm{\theta} \given \bm{x}).
\end{equation*}

\subsubsection{Blocking Parameters}
\label{ssec:blocking}
In the schemes considered so far the proposal, acceptance and rejection of the entire parameter space 
$\bm{\theta}$ happened simultaneously. This approach becomes inefficient for high-dimensional 
problems. Consider that as the dimension of the problem increases, the chances of proposing a value 
$\theta_i^\star$ in the tails of the posterior distribution increases. Increasing the chance of 
proposing a component $\theta_i$ out in the tails of the distribution in turn decreases the 
acceptance rate of the chain and leads to slower convergence.

To overcome this problem the parameter space $\bm{\theta}$ can be split into blocks of parameters 
$\bm{\theta}_1, \ldots, \bm{\theta}_d$ which are proposed and accepted or rejected separately. There 
are no theoretical results which determine how best to block the parameter space, though typically 
blocks are chosen to contain related parameters.

Blocking doesn't reduce the risk of a parameter being proposed in the tails of the distribution, 
however when such a proposal does occur only a subset of all the chains are affected. In this way 
blocking can alleviate the lower acceptance rates that are associated with high dimensionality.

There is, however, an additional computational cost that comes with blocking parameters. Consider 
that if the parameter space $\bm{\theta}$ is partitioned into $d$ blocks, then for each iteration of 
the scheme the acceptance ratio, and therefore the likelihood, proposal density and prior 
distribution must be evaluated $d$ times. In application the practitioner will likely seek some 
compromise between acceptance rate and computation time.

\subsection{Convergence Diagnostics}
\label{ssec:convergence_diagnostics}
Though there are theoretical methods to assess the convergence of chains, it is an attractive idea to 
analyse the output of our schemes in an attempt to assess whether the chains have converged. One of 
the simplest informal methods to assess convergence is to inspect the trace plots of the scheme and 
check for any irregularities. It is also good to use autocorrelation plots to assess autocorrelation 
between samples at different lags.

One way to lower autocorrelation between samples is to thin the output. When thinning, every $k$-th 
sample from a chain is kept and the remaining samples are discarded. Another common technique is to 
allow for a burn-in period. The purpose of a burn-in period is to discard any samples from before the 
chain has converged.